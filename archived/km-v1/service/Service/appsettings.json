{
  "AllowedHosts": "*",
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      // Examples: how to handle logs differently by class
      //      "Microsoft.KernelMemory.Pipeline.Queue.DevTools.SimpleQueue": "Information",
      //      "Microsoft.KernelMemory.Handlers.TextExtractionHandler": "Information",
      //      "Microsoft.KernelMemory.Handlers.TextPartitioningHandler": "Information",
      //      "Microsoft.KernelMemory.Handlers.GenerateEmbeddingsHandler": "Information",
      //      "Microsoft.KernelMemory.Handlers.SaveEmbeddingsHandler": "Information",
      //      "Microsoft.KernelMemory.DocumentStorage.AzureBlobs": "Information",
      //      "Microsoft.KernelMemory.Pipeline.Queue.AzureQueues": "Information",
      "Microsoft.AspNetCore": "Warning"
    },
    "Console": {
      "LogToStandardErrorThreshold": "Critical",
      "FormatterName": "simple",
      "FormatterOptions": {
        "TimestampFormat": "[HH:mm:ss.fff] ",
        "SingleLine": true,
        "UseUtcTimestamp": false,
        "IncludeScopes": false,
        "JsonWriterOptions": {
          "Indented": true
        }
      }
    }
  },
  "KernelMemory": {
    "Service": {
      // Whether to run the web service that allows to upload files and search memory
      // Use these booleans to deploy the web service and the handlers on same/different VMs
      "RunWebService": true,
      // Whether to expose OpenAPI swagger UI at http://127.0.0.1:9001/swagger/index.html
      "OpenApiEnabled": false,
      // The maximum allowed size in MB for the HTTP payload sent to the upload endpoint
      // If not set the solution defaults to 30,000,000 bytes (~28.6 MB)
      // Note: this applies only to KM HTTP service.
      "MaxUploadSizeMb": null,
      // Whether to send a [DONE] message at the end of SSE streams.
      "SendSSEDoneMessage": true,
      // Whether to run the asynchronous pipeline handlers
      // Use these booleans to deploy the web service and the handlers on same/different VMs
      "RunHandlers": true,
      // Handlers to load for callers (use "steps" to choose which handlers
      // to use to process a document during the ingestion)
      "Handlers": {
        // The key, e.g. "extract", is the name used when starting a pipeline with specific steps
        "extract": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.TextExtractionHandler"
        },
        "partition": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.TextPartitioningHandler"
        },
        "gen_embeddings": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.GenerateEmbeddingsHandler"
        },
        "save_records": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.SaveRecordsHandler"
        },
        "summarize": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.SummarizationHandler"
        },
        "delete_generated_files": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.DeleteGeneratedFilesHandler"
        },
        "private_delete_document": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.DeleteDocumentHandler"
        },
        "private_delete_index": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.DeleteIndexHandler"
        },
        "disabled_handler_example": {
          // Setting Class or Assembly to "" in appsettings.Development.json or appsettings.Production.json
          // allows to remove a handler defined in appsettings.json
          "Class": "",
          "Assembly": ""
        }
      }
    },
    "ServiceAuthorization": {
      // Whether clients must provide some credentials to interact with the HTTP API
      "Enabled": false,
      // Currently "APIKey" is the only type supported
      "AuthenticationType": "APIKey",
      // HTTP header name to check
      "HttpHeaderName": "Authorization",
      // Define two separate API Keys, to allow key rotation. Both are active.
      // Keys must be different and case-sensitive, and at least 32 chars long.
      // Contain only alphanumeric chars and allowed symbols.
      // Symbols allowed: . _ - (dot, underscore, minus).
      "AccessKey1": "",
      "AccessKey2": ""
    },
    // "AzureBlobs", "AWSS3", or "SimpleFileStorage"
    "DocumentStorageType": "SimpleFileStorage",
    // "AzureOpenAIText", "OpenAI" or "LlamaSharp"
    "TextGeneratorType": "",
    // Name of the index to use when none is specified
    "DefaultIndexName": "default",
    // Which service to use for content moderation. Optional.
    // Currently supported: AzureAIContentSafety
    "ContentModerationType": "",
    // Data ingestion pipelines configuration.
    "DataIngestion": {
      // - InProcess: in process .NET orchestrator, synchronous/no queues
      // - Distributed: asynchronous queue based orchestrator
      "OrchestrationType": "Distributed",
      "DistributedOrchestration": {
        // "AzureQueues", "RabbitMQ", "SimpleQueues"
        "QueueType": "SimpleQueues"
      },
      // Whether the pipeline generates and saves the vectors/embeddings in the memory DBs.
      // When using a memory DB that automatically generates embeddings internally,
      // or performs semantic search internally anyway, this should be False,
      // and avoid generating embeddings that are not used.
      // Examples:
      // * you are using Azure AI Search "semantic search" without "vector search": in this
      //   case you don't need embeddings because Azure AI Search uses a more advanced approach
      //   internally.
      // * you are using a custom Memory DB connector that generates embeddings on the fly
      //   when writing records and when searching: in this case you don't need the pipeline
      //   to calculate embeddings, because your connector does all the work.
      // * you are using a basic "text search" and a DB without "vector search": in this case
      //   embeddings would be unused, so it's better to disable them to save cost and latency.
      "EmbeddingGenerationEnabled": true,
      // Multiple generators can be used, e.g. for data migration, A/B testing, etc.
      // None of these are used for `ITextEmbeddingGeneration` dependency injection,
      // see Retrieval settings.
      "EmbeddingGeneratorTypes": [
      ],
      // Vectors can be written to multiple storages, e.g. for data migration, A/B testing, etc.
      // "AzureAISearch", "Qdrant", "Postgres", "Redis", "SimpleVectorDb", "SqlServer", etc.
      "MemoryDbTypes": [
        "SimpleVectorDb"
      ],
      // How many memory DB records to insert at once when extracting memories from
      // uploaded documents (used only if the Memory Db supports batching).
      "MemoryDbUpsertBatchSize": 1,
      // "None" or "AzureAIDocIntel"
      "ImageOcrType": "None",
      // Partitioning / Chunking settings
      // How does the partitioning work?
      // * Given a document, text is extracted, and text is split in tokens.
      // * Tokens are merged into chunks, called "partitions", sometimes called "paragraphs"
      // * For each chunk, one (potentially more) memory is generated.
      "TextPartitioning": {
        // Maximum length of chunks in tokens. Tokens depend on the LLM in use.
        "MaxTokensPerParagraph": 1000,
        // How many tokens from a chunk to keep in the following chunk.
        "OverlappingTokens": 100
      },
      // Note: keep the list empty in this file, to avoid unexpected merges
      // with the list defined in appsettings.*.json.
      // If the list is empty, KernelMemoryConfig uses 'Constants.DefaultPipeline'.
      "DefaultSteps": [
        // Default steps defined in 'Constants.DefaultPipeline'
        // "extract",
        // "partition",
        // "gen_embeddings",
        // "save_records",
      ]
    },
    "Retrieval": {
      // "AzureOpenAIEmbedding" or "OpenAI"
      // This is the generator registered for `ITextEmbeddingGeneration` dependency injection.
      "EmbeddingGeneratorType": "",
      // "AzureAISearch", "Qdrant", "Postgres", "Redis", "SimpleVectorDb", "SqlServer", etc.
      "MemoryDbType": "SimpleVectorDb",
      // Search client settings
      "SearchClient": {
        // Maximum number of tokens accepted by the LLM used to generate answers.
        // The number includes the tokens used for the answer, e.g. when using
        // GPT4-32k, set this number to 32768.
        // If the value is not set or less than one, SearchClient will use the
        // max amount of tokens supported by the model in use.
        "MaxAskPromptSize": -1,
        // Maximum number of relevant sources to consider when generating an answer.
        // The value is also used as the max number of results returned by SearchAsync
        // when passing a limit less or equal to zero.
        "MaxMatchesCount": 100,
        // How many tokens to reserve for the answer generated by the LLM.
        // E.g. if the LLM supports max 4000 tokens, and AnswerTokens is 300, then
        // the prompt sent to LLM will contain max 3700 tokens, composed by
        // prompt + question + grounding information retrieved from memory.
        "AnswerTokens": 300,
        // Text to return when the LLM cannot produce an answer.
        "EmptyAnswer": "INFO NOT FOUND",
        // Whether to include duplicate chunks in the RAG prompt.
        "IncludeDuplicateFacts": false,
        // Number between 0 and 2 that controls the randomness of the completion.
        // The higher the temperature, the more random the completion.
        "Temperature": 0,
        // Number between 0 and 2 that controls the diversity of the completion.
        // The higher the TopP, the more diverse the completion.
        "TopP": 0,
        // Number between -2.0 and 2.0. Positive values penalize new tokens based on whether
        // they appear in the text so far, increasing the model's likelihood to talk about
        // new topics.
        "PresencePenalty": 0,
        // Number between -2.0 and 2.0. Positive values penalize new tokens based on their
        // existing frequency in the text so far, decreasing the model's likelihood to repeat
        // the same line verbatim.
        "FrequencyPenalty": 0,
        // Sequences where the completion will stop generating further tokens.
        "StopSequences": [],
        // Modify the likelihood of specified tokens appearing in the completion.
        //"TokenSelectionBiases": { }
        // Whether to check is the generated answers are safe.
        // A content moderation service must be present in the system.
        "UseModerationService": true,
        // Answer to return when AI generates content considered unsafe by the text moderation service.
        "ModeratedAnswer": "Sorry, the generated content contains unsafe or inappropriate information."
      }
    },
    "Services": {
      "Anthropic": {
        "Endpoint": "https://api.anthropic.com",
        "EndpointVersion": "2023-06-01",
        "ApiKey": "",
        // See https://docs.anthropic.com/claude/docs/models-overview for list of models and details
        "TextModelName": "claude-3-haiku-20240307",
        // Supported values: "p50k", "cl100k", "o200k". Leave it empty if unsure.
        "Tokenizer": "cl100k",
        // How many tokens the model can receive in input and generate in output
        // See https://docs.anthropic.com/claude/docs/models-overview
        "MaxTokenIn": 200000,
        "MaxTokenOut": 4096,
        "DefaultSystemPrompt": "You are an assistant that will answer user query based on a context",
        "HttpClientName": ""
      },
      "AWSS3": {
        // "AccessKey" or "CredentialChain". For other options see <AWSS3Config>.
        "Auth": "AccessKey",
        // AccessKey ID, required when using AccessKey auth
        // Note: you can use an env var 'KernelMemory__Services__AWSS3__AccessKey' to set this
        "AccessKey": "",
        // SecretAccessKey, required when using AccessKey auth
        // Note: you can use an env var 'KernelMemory__Services__AWSS3__SecretAccessKey' to set this
        "SecretAccessKey": "",
        // Required bucket name where to create directories and upload files.
        // Note: you can use an env var 'KernelMemory__Services__AWSS3__BucketName' to set this
        "BucketName": ""
        // Allows to specify a custom AWS or a compatible endpoint
        // Examples: "https://s3.amazonaws.com", "https://s3.us-west-2.amazonaws.com", "http://127.0.0.1:9444"
        // Note: you can use an env var 'KernelMemory__Services__AWSS3__Endpoint' to set this
        // Note: you can test locally using S3 Ninja https://s3ninja.net
        // "Endpoint": "https://s3.amazonaws.com"
      },
      "AzureAIContentSafety": {
        // "ApiKey" or "AzureIdentity". For other options see <AzureAIContentSafetyConfig>.
        // AzureIdentity: use automatic Entra (AAD) authentication mechanism.
        //   When the service is on sovereign clouds you can use the AZURE_AUTHORITY_HOST env var to
        //   set the authority host. See https://learn.microsoft.com/dotnet/api/overview/azure/identity-readme
        //   You can test locally using the AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET env vars.
        "Auth": "AzureIdentity",
        "Endpoint": "https://<...>",
        "APIKey": "",
        "GlobalSafetyThreshold": 0.0,
        "IgnoredWords": []
      },
      "AzureAIDocIntel": {
        // "APIKey" or "AzureIdentity".
        // AzureIdentity: use automatic Entra (AAD) authentication mechanism.
        //   When the service is on sovereign clouds you can use the AZURE_AUTHORITY_HOST env var to
        //   set the authority host. See https://learn.microsoft.com/dotnet/api/overview/azure/identity-readme
        //   You can test locally using the AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET env vars.
        "Auth": "AzureIdentity",
        // Optional when Auth == AzureIdentity. Leave it null to use the default.
        // When the service is on sovereign clouds, this setting might be necessary to configure Entra auth tokens.
        // See https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/formrecognizer/Azure.AI.FormRecognizer/src/DocumentAnalysisAudience.cs
        "AzureIdentityAudience": null,
        // Required when Auth == APIKey
        "APIKey": "",
        "Endpoint": ""
      },
      "AzureAISearch": {
        // "ApiKey" or "AzureIdentity". For other options see <AzureAISearchConfig>.
        // AzureIdentity: use automatic Entra (AAD) authentication mechanism.
        //   When the service is on sovereign clouds you can use the AZURE_AUTHORITY_HOST env var to
        //   set the authority host. See https://learn.microsoft.com/dotnet/api/overview/azure/identity-readme
        //   You can test locally using the AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET env vars.
        "Auth": "AzureIdentity",
        // Optional when Auth == AzureIdentity. Leave it null to use the default.
        // When the service is on sovereign clouds, this setting might be necessary to configure Entra auth tokens.
        // See https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/search/Azure.Search.Documents/src/SearchAudience.cs
        "AzureIdentityAudience": null,
        // Required when Auth == APIKey
        "APIKey": "",
        "Endpoint": "https://<...>",
        // Hybrid search is not enabled by default. Note that when using hybrid search
        // relevance scores are different, usually lower, than when using just vector search
        "UseHybridSearch": false,
        // Helps improve relevance score consistency for search services with multiple replicas by
        // attempting to route a given request to the same replica for that session. Use this when
        // favoring consistent scoring over lower latency. Can adversely affect performance.
        //
        // Whether to use sticky sessions, which can help getting more consistent results.
        // When using sticky sessions, a best-effort attempt will be made to target the same replica set.
        // Be wary that reusing the same replica repeatedly can interfere with the load balancing of
        // the requests across replicas and adversely affect the performance of the search service.
        //
        // See https://learn.microsoft.com/rest/api/searchservice/documents/search-post?view=rest-searchservice-2024-07-01&tabs=HTTP#request-body
        "UseStickySessions": false
      },
      "AzureBlobs": {
        // "ConnectionString" or "AzureIdentity". For other options see <AzureBlobConfig>.
        // AzureIdentity: use automatic Entra (AAD) authentication mechanism.
        //   When the service is on sovereign clouds you can use the AZURE_AUTHORITY_HOST env var to
        //   set the authority host. See https://learn.microsoft.com/dotnet/api/overview/azure/identity-readme
        //   You can test locally using the AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET env vars.
        "Auth": "AzureIdentity",
        // Optional when Auth == AzureIdentity. Leave it null to use the default.
        // When the service is on sovereign clouds, this setting might be necessary to configure Entra auth tokens.
        "AzureIdentityAudience": null,
        // Azure Storage account name, required when using AzureIdentity auth
        // Note: you can use an env var 'KernelMemory__Services__AzureBlobs__Account' to set this
        "Account": "",
        // Container where to create directories and upload files
        "Container": "smemory",
        // Required when Auth == ConnectionString
        // Note: you can use an env var 'KernelMemory__Services__AzureBlobs__ConnectionString' to set this
        "ConnectionString": "",
        // Setting used only for country clouds
        "EndpointSuffix": "core.windows.net"
      },
      "AzureOpenAIEmbedding": {
        // "ApiKey" or "AzureIdentity"
        // AzureIdentity: use automatic Entra (AAD) authentication mechanism.
        //   You can test locally using the AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET env vars.
        "Auth": "AzureIdentity",
        // Optional when Auth == AzureIdentity. Leave it null to use the default.
        // When the service is on sovereign clouds, this setting might be necessary to configure Entra auth tokens.
        // - https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/openai/Azure.AI.OpenAI/README.md
        // - https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/openai/Azure.AI.OpenAI/src/Custom/AzureOpenAIAudience.cs
        "AzureIdentityAudience": null,
        "Endpoint": "https://<...>.openai.azure.com/",
        "APIKey": "",
        // Your Azure Deployment name
        "Deployment": "",
        // The max number of tokens supported by model deployed
        // See https://learn.microsoft.com/azure/ai-services/openai/concepts/models
        "MaxTokenTotal": 8191,
        // Which tokenizer to use to correctly measure the size of chunks.
        // Supported values: "p50k", "cl100k", "o200k". Leave it empty if unsure.
        // - Use p50k for the old text-davinci-003 models
        // - Use cl100k for the old gpt-3.4 and gpt-4 family, and for text embedding models
        // - Use o200k for the most recent gpt-4o family
        "Tokenizer": "cl100k",
        // The number of dimensions output embeddings should have.
        // Only supported in "text-embedding-3" and later models developed with
        // MRL, see https://arxiv.org/abs/2205.13147
        "EmbeddingDimensions": null,
        // How many embeddings to calculate in parallel. The max value depends on
        // the model and deployment in use.
        // See https://learn.microsoft.com/azure/ai-services/openai/reference#embeddings
        "MaxEmbeddingBatchSize": 1,
        // How many times to retry in case of throttling.
        "MaxRetries": 10,
        // Thumbprints of certificates that should be trusted for HTTPS requests when SSL policy errors are detected.
        // This should only be used for local development when using a proxy to call the OpenAI endpoints.
        "TrustedCertificateThumbprints": []
      },
      "AzureOpenAIText": {
        // "ApiKey" or "AzureIdentity"
        // AzureIdentity: use automatic Entra (AAD) authentication mechanism.
        //   You can test locally using the AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET env vars.
        "Auth": "AzureIdentity",
        // Optional when Auth == AzureIdentity. Leave it null to use the default.
        // When the service is on sovereign clouds, this setting might be necessary to configure Entra auth tokens.
        // - https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/openai/Azure.AI.OpenAI/README.md
        // - https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/openai/Azure.AI.OpenAI/src/Custom/AzureOpenAIAudience.cs
        "AzureIdentityAudience": null,
        "Endpoint": "https://<...>.openai.azure.com/",
        "APIKey": "",
        "Deployment": "",
        // The max number of tokens supported by model deployed
        // See https://learn.microsoft.com/azure/ai-services/openai/concepts/models
        "MaxTokenTotal": 16384,
        // Which tokenizer to use to correctly measure the size of chunks.
        // Supported values: "p50k", "cl100k", "o200k". Leave it empty if unsure.
        // - Use p50k for the old text-davinci-003 models
        // - Use cl100k for the old gpt-3.4 and gpt-4 family, and for text embedding models
        // - Use o200k for the most recent gpt-4o family
        "Tokenizer": "o200k",
        // "ChatCompletion" or "TextCompletion"
        "APIType": "ChatCompletion",
        // How many times to retry in case of throttling.
        "MaxRetries": 10,
        // Thumbprints of certificates that should be trusted for HTTPS requests when SSL policy errors are detected.
        // This should only be used for local development when using a proxy to call the OpenAI endpoints.
        "TrustedCertificateThumbprints": []
      },
      "AzureQueues": {
        // "ConnectionString" or "AzureIdentity". For other options see <AzureQueueConfig>.
        // AzureIdentity: use automatic Entra (AAD) authentication mechanism.
        //   When the service is on sovereign clouds you can use the AZURE_AUTHORITY_HOST env var to
        //   set the authority host. See https://learn.microsoft.com/dotnet/api/overview/azure/identity-readme
        //   You can test locally using the AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET env vars.
        "Auth": "AzureIdentity",
        // Optional when Auth == AzureIdentity. Leave it null to use the default.
        // When the service is on sovereign clouds, this setting might be necessary to configure Entra auth tokens.
        "AzureIdentityAudience": null,
        // Azure Storage account name, required when using AzureIdentity auth
        // Note: you can use an env var 'KernelMemory__Services__AzureQueue__Account' to set this
        "Account": "",
        // Required when Auth == ConnectionString
        // Note: you can use an env var 'KernelMemory__Services__AzureQueue__ConnectionString' to set this
        "ConnectionString": "",
        // Setting used only for country clouds
        "EndpointSuffix": "core.windows.net",
        // How often to check if there are new messages
        "PollDelayMsecs": 100,
        // How many messages to fetch at a time
        "FetchBatchSize": 3,
        // How long to lock messages once fetched. Azure Queue default is 30 secs
        "FetchLockSeconds": 300,
        // How many times to dequeue a messages and process before moving it to a poison queue
        "MaxRetriesBeforePoisonQueue": 20,
        // Suffix used for the poison queues.
        "PoisonQueueSuffix": "-poison"
      },
      "Elasticsearch": {
        // SHA-256 fingerprint. When running the docker image this is printed after starting the server
        // See https://www.elastic.co/guide/en/elasticsearch/reference/current/configuring-stack-security.html#_use_the_ca_fingerprint_5
        "CertificateFingerPrint": "",
        // e.g. https://localhost:9200
        "Endpoint": "",
        // e.g. "elastic"
        "UserName": "",
        "Password": "",
        "IndexPrefix": "",
        "ShardCount": 1,
        "Replicas": 0
      },
      "LlamaSharp": {
        "TextModel": {
          // path to file, e.g. "llama-2-7b-chat.Q6_K.gguf"
          "ModelPath": "",
          // Max number of tokens supported by the model
          "MaxTokenTotal": 4096
          // Optional parameters
          // "GpuLayerCount": 32,
        },
        "EmbeddingModel": {
          // path to file, e.g. "nomic-embed-text-v1.5.Q8_0.gguf"
          "ModelPath": "",
          // Max number of tokens supported by the model
          "MaxTokenTotal": 4096
          // Optional parameters
          // "GpuLayerCount": 32,
        }
      },
      "MongoDbAtlas": {
        "ConnectionString": "mongodb://root:root@localhost:27777/?authSource=admin",
        "DatabaseName": "KernelMemory",
        "UseSingleCollectionForVectorSearch": false
      },
      "Ollama": {
        "Endpoint": "http://localhost:11434",
        "TextModel": {
          "ModelName": "phi3:medium-128k",
          // Supported values: "p50k", "cl100k", "o200k". Leave it empty if unsure.
          "Tokenizer": "cl100k",
          "MaxTokenTotal": 131072,
          // How many requests can be processed in parallel
          "MaxBatchSize": 1
          //// Enable Mirostat sampling for controlling perplexity.
          //// (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)
          //"MiroStat": 0,
          //// Influences how quickly the algorithm responds to feedback from the
          //// generated text. A lower learning rate will result in slower adjustments,
          //// while a higher learning rate will make the algorithm more responsive.
          //// (Default: 0.1)
          //"MiroStatEta": 0.1,
          //// Controls the balance between coherence and diversity of the output.
          //// A lower value will result in more focused and coherent text.
          //// (Default: 5.0)
          //"MiroStatTau": 5.0,
          //// Sets the size of the context window used to generate the next token.
          //// (Default: 2048)
          //"NumCtx": 2048,
          //// The number of GQA groups in the transformer layer. Required for some
          //// models, for example it is 8 for llama2:70b
          //"NumGqa": null,
          //// The number of layers to send to the GPU(s). On macOS it defaults to
          //// 1 to enable metal support, 0 to disable.
          //"NumGpu": null,
          //// Sets the number of threads to use during computation. By default,
          //// Ollama will detect this for optimal performance.
          //// It is recommended to set this value to the number of physical CPU cores
          //// your system has (as opposed to the logical number of cores).
          //"NumThread": null,
          //// Sets how far back for the model to look back to prevent repetition.
          //// (Default: 64, 0 = disabled, -1 = num_ctx)
          //"RepeatLastN": null,
          //// Sets the random number seed to use for generation.
          //// Setting this to a specific number will make the model generate the same
          //// text for the same prompt. (Default: 0)
          //"Seed": 0,
          //// Tail free sampling is used to reduce the impact of less probable
          //// tokens from the output. A higher value (e.g., 2.0) will reduce the
          //// impact more, while a value of 1.0 disables this setting. (default: 1)
          //"TfsZ": 1.0,
          //// Maximum number of tokens to predict when generating text.
          //// (Default: 128, -1 = infinite generation, -2 = fill context)
          //"NumPredict": 128,
          //// Reduces the probability of generating nonsense. A higher value
          //// (e.g. 100) will give more diverse answers, while a lower value (e.g. 10)
          //// will be more conservative. (Default: 40)
          //"TopK": 40,
          //// Alternative to the top_p, and aims to ensure a balance of quality and variety.min_p represents the minimum
          //// probability for a token to be considered, relative to the probability of the most likely token.For
          //// example, with min_p=0.05 and the most likely token having a probability of 0.9, logits with a value less
          //// than 0.05*0.9=0.045 are filtered out. (Default: 0.0)
          //"MinP": 0.0
        },
        "EmbeddingModel": {
          "ModelName": "nomic-embed-text",
          // Supported values: "p50k", "cl100k", "o200k". Leave it empty if unsure.
          "Tokenizer": "cl100k",
          "MaxTokenTotal": 2048,
          // How many requests can be processed in parallel
          "MaxBatchSize": 1
          //// Enable Mirostat sampling for controlling perplexity.
          //// (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)
          //"MiroStat": 0,
          //// Influences how quickly the algorithm responds to feedback from the
          //// generated text. A lower learning rate will result in slower adjustments,
          //// while a higher learning rate will make the algorithm more responsive.
          //// (Default: 0.1)
          //"MiroStatEta": 0.1,
          //// Controls the balance between coherence and diversity of the output.
          //// A lower value will result in more focused and coherent text.
          //// (Default: 5.0)
          //"MiroStatTau": 5.0,
          //// Sets the size of the context window used to generate the next token.
          //// (Default: 2048)
          //"NumCtx": 2048,
          //// The number of GQA groups in the transformer layer. Required for some
          //// models, for example it is 8 for llama2:70b
          //"NumGqa": null,
          //// The number of layers to send to the GPU(s). On macOS it defaults to
          //// 1 to enable metal support, 0 to disable.
          //"NumGpu": null,
          //// Sets the number of threads to use during computation. By default,
          //// Ollama will detect this for optimal performance.
          //// It is recommended to set this value to the number of physical CPU cores
          //// your system has (as opposed to the logical number of cores).
          //"NumThread": null,
          //// Sets how far back for the model to look back to prevent repetition.
          //// (Default: 64, 0 = disabled, -1 = num_ctx)
          //"RepeatLastN": null,
          //// Sets the random number seed to use for generation.
          //// Setting this to a specific number will make the model generate the same
          //// text for the same prompt. (Default: 0)
          //"Seed": 0,
          //// Tail free sampling is used to reduce the impact of less probable
          //// tokens from the output. A higher value (e.g., 2.0) will reduce the
          //// impact more, while a value of 1.0 disables this setting. (default: 1)
          //"TfsZ": 1.0,
          //// Maximum number of tokens to predict when generating text.
          //// (Default: 128, -1 = infinite generation, -2 = fill context)
          //"NumPredict": 128,
          //// Reduces the probability of generating nonsense. A higher value
          //// (e.g. 100) will give more diverse answers, while a lower value (e.g. 10)
          //// will be more conservative. (Default: 40)
          //"TopK": 40,
          //// Alternative to the top_p, and aims to ensure a balance of quality and variety.min_p represents the minimum
          //// probability for a token to be considered, relative to the probability of the most likely token.For
          //// example, with min_p=0.05 and the most likely token having a probability of 0.9, logits with a value less
          //// than 0.05*0.9=0.045 are filtered out. (Default: 0.0)
          //"MinP": 0.0
        }
      },
      "OpenAI": {
        // Name of the model used to generate text (text completion or chat completion)
        "TextModel": "gpt-4o-mini",
        // The max number of tokens supported by the text model.
        "TextModelMaxTokenTotal": 16384,
        // Supported values: "p50k", "cl100k", "o200k". Leave it empty for autodetect.
        "TextModelTokenizer": "",
        // What type of text generation, by default autodetect using the model name.
        // Possible values: "Auto", "TextCompletion", "Chat"
        "TextGenerationType": "Auto",
        // Name of the model used to generate text embeddings
        "EmbeddingModel": "text-embedding-ada-002",
        // The max number of tokens supported by the embedding model
        // See https://platform.openai.com/docs/guides/embeddings/what-are-embeddings
        "EmbeddingModelMaxTokenTotal": 8191,
        // Supported values: "p50k", "cl100k", "o200k". Leave it empty for autodetect.
        "EmbeddingModelTokenizer": "",
        // OpenAI API Key
        "APIKey": "",
        // OpenAI Organization ID (usually empty, unless you have multiple accounts on different orgs)
        "OrgId": "",
        // Endpoint to use. By default the system uses 'https://api.openai.com/v1'.
        // Change this to use proxies or services compatible with OpenAI HTTP protocol like LM Studio.
        "Endpoint": "",
        // How many times to retry in case of throttling
        "MaxRetries": 10,
        // The number of dimensions output embeddings should have.
        // Only supported in "text-embedding-3" and later models developed with
        // MRL, see https://arxiv.org/abs/2205.13147
        "EmbeddingDimensions": null,
        // How many embeddings to calculate in parallel.
        // See https://platform.openai.com/docs/api-reference/embeddings/create
        "MaxEmbeddingBatchSize": 100
      },
      "Postgres": {
        // Postgres instance connection string
        "ConnectionString": "Host=localhost;Port=5432;Username=public;Password=;Database=public",
        // Mandatory prefix to add to the name of table managed by KM,
        // e.g. to exclude other tables in the same schema.
        "TableNamePrefix": "km-"
      },
      "Qdrant": {
        // Qdrant endpoint
        "Endpoint": "http://127.0.0.1:6333",
        // Qdrant API key, e.g. when using Qdrant cloud
        "APIKey": ""
      },
      "RabbitMQ": {
        "Host": "127.0.0.1",
        "Port": "5672",
        "Username": "user",
        "Password": "",
        "VirtualHost": "/",
        "MessageTTLSecs": 3600,
        "SslEnabled": false,
        // How many messages to process asynchronously at a time, in each queue
        "ConcurrentThreads": 4,
        // How many messages to fetch at a time from each queue
        // The value should be higher than ConcurrentThreads
        "PrefetchCount": 8,
        // How many times to dequeue a messages and process before moving it to a poison queue
        // Note: this value cannot be changed after queues have been created. In such case
        //       you might need to drain all queues, delete them, and restart the ingestion service(s).
        "MaxRetriesBeforePoisonQueue": 20,
        // How long to wait before putting a message back to the queue in case of failure
        "DelayBeforeRetryingMsecs": 500,
        // Suffix used for the poison queues.
        "PoisonQueueSuffix": "-poison"
      },
      "Redis": {
        // Redis connection string, e.g. "localhost:6379,password=..."
        "ConnectionString": "",
        // List of tags that clients will use to filter memories. When using Redis,
        // the list of tags must be configured, for data to be saved correctly.
        "Tags": {
          "type": ",",
          "user": ",",
          "ext": ","
        }
      },
      "SimpleFileStorage": {
        // Options: "Disk" or "Volatile". Volatile data is lost after each execution.
        "StorageType": "Volatile",
        // Directory where files are stored.
        "Directory": "_files"
      },
      "SimpleQueues": {
        // Options: "Disk" or "Volatile". Volatile data is lost after each execution.
        "StorageType": "Volatile",
        // Directory where files are stored.
        "Directory": "_queues",
        // How often to check if there are new messages.
        "PollDelayMsecs": 100,
        // How often to dispatch messages in the queue.
        "DispatchFrequencyMsecs": 100,
        // How many messages to fetch at a time.
        "FetchBatchSize": 3,
        // How long to lock messages once fetched.
        "FetchLockSeconds": 300,
        // How many times to retry processing a failing message.
        "MaxRetriesBeforePoisonQueue": 1,
        // Suffix used for the poison queue directories
        "PoisonQueueSuffix": ".poison"
      },
      "SimpleVectorDb": {
        // Options: "Disk" or "Volatile". Volatile data is lost after each execution.
        "StorageType": "Volatile",
        // Directory where files are stored.
        "Directory": "_vectors"
      },
      "SqlServer": {
        // MS SQL Server Connection string, e.g.
        //    "Server=tcp:127.0.0.1,1433;Initial Catalog=master;Persist Security Info=False;User ID=sa;Password=00_CHANGE_ME_00;MultipleActiveResultSets=False;TrustServerCertificate=True;Connection Timeout=30;"
        "ConnectionString": "",
        "Schema": "dbo",
        "MemoryCollectionTableName": "KMCollections",
        "MemoryTableName": "KMMemories",
        "EmbeddingsTableName": "KMEmbeddings",
        "TagsTableName": "KMMemoriesTags",
        // See https://learn.microsoft.com/sql/relational-databases/vectors/vectors-sql-server?view=azuresqldb-current
        "UseNativeVectorSearch": false,
        "VectorSize": 1536
      }
    }
  }
}
