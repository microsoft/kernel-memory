{
  "KernelMemory": {
    "ServiceAuthorization": {
      "Endpoint": "http://127.0.0.1:9001/",
      "AccessKey": ""
    },
    "Services": {
      "SimpleVectorDb": {
        // Options: "Disk", "Volatile"
        "StorageType": "Volatile",
        "Directory": "tmp-vectors"
      },
      "AzureAISearch": {
        // "ApiKey" or "AzureIdentity". For other options see <AzureAISearchConfig>.
        // AzureIdentity: use automatic Entra (AAD) authentication mechanism.
        //   When the service is on sovereign clouds you can use the AZURE_AUTHORITY_HOST env var to
        //   set the authority host. See https://learn.microsoft.com/dotnet/api/overview/azure/identity-readme
        //   You can test locally using the AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET env vars.
        "Auth": "AzureIdentity",
        "Endpoint": "https://<...>",
        "APIKey": ""
      },
      "Qdrant": {
        "Endpoint": "http://127.0.0.1:6333",
        "APIKey": ""
      },
      "AzureOpenAIText": {
        // "ApiKey" or "AzureIdentity"
        // AzureIdentity: use automatic Entra (AAD) authentication mechanism.
        //   When the service is on sovereign clouds you can use the AZURE_AUTHORITY_HOST env var to
        //   set the authority host. See https://learn.microsoft.com/dotnet/api/overview/azure/identity-readme
        //   You can test locally using the AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET env vars.
        "Auth": "AzureIdentity",
        "Endpoint": "https://<...>.openai.azure.com/",
        "APIKey": "",
        "Deployment": "",
        // The max number of tokens supported by model deployed
        // See https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models
        "MaxTokenTotal": 16384,
        // "ChatCompletion" or "TextCompletion"
        "APIType": "ChatCompletion",
        // How many times to retry in case of throttling.
        "MaxRetries": 10
      },
      "OpenAI": {
        "APIKey": ""
      },
      "LlamaSharp": {
        "TextModel": {
          // path to file, e.g. "llama-2-7b-chat.Q6_K.gguf"
          "ModelPath": "",
          // Max number of tokens supported by the model
          "MaxTokenTotal": 4096
          // Optional parameters
          // "GpuLayerCount": 32,
        },
        "EmbeddingModel": {
          // path to file, e.g. "nomic-embed-text-v1.5.Q8_0.gguf"
          "ModelPath": "",
          // Max number of tokens supported by the model
          "MaxTokenTotal": 4096
          // Optional parameters
          // "GpuLayerCount": 32,
        }
      }
    }
  },
  "Logging": {
    "LogLevel": {
      "Default": "Information"
    }
  }
}