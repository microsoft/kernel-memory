{
  "AllowedHosts": "*",
  "Kestrel": {
    "Endpoints": {
      "Http": {
        "Url": "http://*:9001"
      }
      // "Https": {
      //  "Url": "https://*:9002"
      // }
    }
  },
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      // Examples: how to handle logs differently by class
      //      "Microsoft.KernelMemory.Pipeline.Queue.DevTools.SimpleQueue": "Information",
      //      "Microsoft.KernelMemory.Handlers.TextExtractionHandler": "Information",
      //      "Microsoft.KernelMemory.Handlers.TextPartitioningHandler": "Information",
      //      "Microsoft.KernelMemory.Handlers.GenerateEmbeddingsHandler": "Information",
      //      "Microsoft.KernelMemory.Handlers.SaveEmbeddingsHandler": "Information",
      //      "Microsoft.KernelMemory.DocumentStorage.AzureBlobs": "Information",
      //      "Microsoft.KernelMemory.Pipeline.Queue.AzureQueues": "Information",
      "Microsoft.AspNetCore": "Warning"
    },
    "Console": {
      "LogToStandardErrorThreshold": "Critical",
      "FormatterName": "simple",
      "FormatterOptions": {
        "TimestampFormat": "[HH:mm:ss.fff] ",
        "SingleLine": true,
        "UseUtcTimestamp": false,
        "IncludeScopes": false,
        "JsonWriterOptions": {
          "Indented": true
        }
      }
    }
  },
  "KernelMemory": {
    "Service": {
      // Whether to run the web service that allows to upload files and search memory
      // Use these booleans to deploy the web service and the handlers on same/different VMs
      "RunWebService": true,
      // Whether to expose OpenAPI swagger UI at http://127.0.0.1:9001/swagger/index.html
      "OpenApiEnabled": false,
      // Whether to run the asynchronous pipeline handlers
      // Use these booleans to deploy the web service and the handlers on same/different VMs
      "RunHandlers": true,
      // Handlers to load for callers (use "steps" to choose which handlers
      // to use to process a document during the ingestion)
      "Handlers": {
        // The key, e.g. "extract", is the name used when starting a pipeline with specific steps
        "extract": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.TextExtractionHandler"
        },
        "partition": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.TextPartitioningHandler"
        },
        "gen_embeddings": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.GenerateEmbeddingsHandler"
        },
        "save_records": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.SaveRecordsHandler"
        },
        "summarize": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.SummarizationHandler"
        },
        "delete_generated_files": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.DeleteGeneratedFilesHandler"
        },
        "private_delete_document": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.DeleteDocumentHandler"
        },
        "private_delete_index": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.DeleteIndexHandler"
        },
        "disabled_handler_example": {
          // Setting Class or Assembly to "" in appsettings.Development.json or appsettings.Production.json
          // allows to remove a handler defined in appsettings.json
          "Class": "",
          "Assembly": ""
        }
      }
    },
    "ServiceAuthorization": {
      // Whether clients must provide some credentials to interact with the HTTP API
      "Enabled": false,
      // Currently "APIKey" is the only type supported
      "AuthenticationType": "APIKey",
      // HTTP header name to check
      "HttpHeaderName": "Authorization",
      // Define two separate API Keys, to allow key rotation. Both are active.
      // Keys must be different and case-sensitive, and at least 32 chars long.
      // Contain only alphanumeric chars and allowed symbols.
      // Symbols allowed: . _ - (dot, underscore, minus).
      "AccessKey1": "",
      "AccessKey2": ""
    },
    // "AzureBlobs" or "SimpleFileStorage"
    "DocumentStorageType": "SimpleFileStorage",
    // "AzureOpenAIText", "OpenAI" or "LlamaSharp"
    "TextGeneratorType": "",
    // Name of the index to use when none is specified
    "DefaultIndexName": "default",
    // Data ingestion pipelines configuration.
    "DataIngestion": {
      // - InProcess: in process .NET orchestrator, synchronous/no queues
      // - Distributed: asynchronous queue based orchestrator
      "OrchestrationType": "Distributed",
      "DistributedOrchestration": {
        // "AzureQueues", "RabbitMQ", "SimpleQueues"
        "QueueType": "SimpleQueues"
      },
      // Whether the pipeline generates and saves the vectors/embeddings in the memory DBs.
      // When using a memory DB that automatically generates embeddings internally,
      // or performs semantic search internally anyway, this should be False,
      // and avoid generating embeddings that are not used.
      // Examples:
      // * you are using Azure AI Search "semantic search" without "vector search": in this
      //   case you don't need embeddings because Azure AI Search uses a more advanced approach
      //   internally.
      // * you are using a custom Memory DB connector that generates embeddings on the fly
      //   when writing records and when searching: in this case you don't need the pipeline
      //   to calculate embeddings, because your connector does all the work.
      // * you are using a basic "text search" and a DB without "vector search": in this case
      //   embeddings would be unused, so it's better to disable them to save cost and latency.
      "EmbeddingGenerationEnabled": true,
      // Multiple generators can be used, e.g. for data migration, A/B testing, etc.
      // None of these are used for `ITextEmbeddingGeneration` dependency injection,
      // see Retrieval settings.
      "EmbeddingGeneratorTypes": [
      ],
      // Vectors can be written to multiple storages, e.g. for data migration, A/B testing, etc.
      // "AzureAISearch", "Qdrant", "Postgres", "Redis", "SimpleVectorDb", "SqlServer", etc.
      "MemoryDbTypes": [
        "SimpleVectorDb"
      ],
      // How many memory DB records to insert at once when extracting memories from
      // uploaded documents (used only if the Memory Db supports batching).
      "MemoryDbUpsertBatchSize": 1,
      // "None" or "AzureAIDocIntel"
      "ImageOcrType": "None",
      // Partitioning / Chunking settings
      // How does the partitioning work?
      // * Given a document, text is extracted, and text is split in sentences, called "lines of text".
      // * Sentences are merged into paragraphs, called "partitions".
      // * For each partition, one (potentially more) memory is generated.
      "TextPartitioning": {
        // Maximum length of lines of text (aka sentences), in tokens. Tokens depend on the LLM in use.
        // Sentences are grouped into paragraphs, see the next setting.
        "MaxTokensPerLine": 300,
        // Maximum length of paragraphs (aka partitions), in tokens. Tokens depend on the LLM in use.
        "MaxTokensPerParagraph": 1000,
        // How many tokens from a paragraph to keep in the following paragraph.
        "OverlappingTokens": 100
      },
      // Note: keep the list empty in this file, to avoid unexpected merges
      // with the list defined in appsettings.*.json.
      // If the list is empty, KernelMemoryConfig uses 'Constants.DefaultPipeline'.
      "DefaultSteps": [
        // Default steps defined in 'Constants.DefaultPipeline'
        // "extract",
        // "partition",
        // "gen_embeddings",
        // "save_records",
      ]
    },
    "Retrieval": {
      // "AzureOpenAIEmbedding" or "OpenAI"
      // This is the generator registered for `ITextEmbeddingGeneration` dependency injection.
      "EmbeddingGeneratorType": "",
      // "AzureAISearch", "Qdrant", "Postgres", "Redis", "SimpleVectorDb", "SqlServer", etc.
      "MemoryDbType": "SimpleVectorDb",
      // Search client settings
      "SearchClient": {
        // Maximum number of tokens accepted by the LLM used to generate answers.
        // The number includes the tokens used for the answer, e.g. when using
        // GPT4-32k, set this number to 32768.
        // If the value is not set or less than one, SearchClient will use the
        // max amount of tokens supported by the model in use.
        "MaxAskPromptSize": -1,
        // Maximum number of relevant sources to consider when generating an answer.
        // The value is also used as the max number of results returned by SearchAsync
        // when passing a limit less or equal to zero.
        "MaxMatchesCount": 100,
        // How many tokens to reserve for the answer generated by the LLM.
        // E.g. if the LLM supports max 4000 tokens, and AnswerTokens is 300, then
        // the prompt sent to LLM will contain max 3700 tokens, composed by
        // prompt + question + grounding information retrieved from memory.
        "AnswerTokens": 300,
        // Text to return when the LLM cannot produce an answer.
        "EmptyAnswer": "INFO NOT FOUND",
        // Number between 0 and 2 that controls the randomness of the completion.
        // The higher the temperature, the more random the completion.
        "Temperature": 0,
        // Number between 0 and 2 that controls the diversity of the completion.
        // The higher the TopP, the more diverse the completion.
        "TopP": 0,
        // Number between -2.0 and 2.0. Positive values penalize new tokens based on whether
        // they appear in the text so far, increasing the model's likelihood to talk about
        // new topics.
        "PresencePenalty": 0,
        // Number between -2.0 and 2.0. Positive values penalize new tokens based on their
        // existing frequency in the text so far, decreasing the model's likelihood to repeat
        // the same line verbatim.
        "FrequencyPenalty": 0,
        // Sequences where the completion will stop generating further tokens.
        "StopSequences": []
        // Modify the likelihood of specified tokens appearing in the completion.
        //"TokenSelectionBiases": { }
      }
    },
    "Services": {
      "Anthropic": {
        "Endpoint": "https://api.anthropic.com",
        "EndpointVersion": "2023-06-01",
        "ApiKey": "",
        // See https://docs.anthropic.com/claude/docs/models-overview for list of models and details
        "TextModelName": "claude-3-haiku-20240307",
        // How many tokens the model can receive in input and generate in output
        // See https://docs.anthropic.com/claude/docs/models-overview
        "MaxTokenIn": 200000,
        "MaxTokenOut": 4096,
        "DefaultSystemPrompt": "You are an assistant that will answer user query based on a context",
        "HttpClientName": ""
      },
      "AzureAISearch": {
        // "ApiKey" or "AzureIdentity". For other options see <AzureAISearchConfig>.
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        "Endpoint": "https://<...>",
        "APIKey": "",
        // Hybrid search is not enabled by default. Note that when using hybrid search
        // relevance scores are different, usually lower, than when using just vector search
        "UseHybridSearch": false
      },
      "AzureAIDocIntel": {
        // "APIKey" or "AzureIdentity".
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        // Required when Auth == APIKey
        "APIKey": "",
        "Endpoint": ""
      },
      "AzureBlobs": {
        // "ConnectionString" or "AzureIdentity". For other options see <AzureBlobConfig>.
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        // Azure Storage account name, required when using AzureIdentity auth
        // Note: you can use an env var 'KernelMemory__Services__AzureBlobs__Account' to set this
        "Account": "",
        // Container where to create directories and upload files
        "Container": "smemory",
        // Required when Auth == ConnectionString
        // Note: you can use an env var 'KernelMemory__Services__AzureBlobs__ConnectionString' to set this
        "ConnectionString": "",
        // Setting used only for country clouds
        "EndpointSuffix": "core.windows.net"
      },
      "AzureOpenAIEmbedding": {
        // "ApiKey" or "AzureIdentity"
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        "Endpoint": "https://<...>.openai.azure.com/",
        "APIKey": "",
        "Deployment": "",
        // The max number of tokens supported by model deployed
        // See https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models
        "MaxTokenTotal": 8191,
        // The number of dimensions output embeddings should have.
        // Only supported in "text-embedding-3" and later models developed with
        // MRL, see https://arxiv.org/abs/2205.13147
        "EmbeddingDimensions": null
      },
      "AzureOpenAIText": {
        // "ApiKey" or "AzureIdentity"
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        "Endpoint": "https://<...>.openai.azure.com/",
        "APIKey": "",
        "Deployment": "",
        // The max number of tokens supported by model deployed
        // See https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models
        "MaxTokenTotal": 16384,
        // "ChatCompletion" or "TextCompletion"
        "APIType": "ChatCompletion",
        "MaxRetries": 10
      },
      "AzureQueues": {
        // "ConnectionString" or "AzureIdentity". For other options see <AzureQueueConfig>.
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        // Azure Storage account name, required when using AzureIdentity auth
        // Note: you can use an env var 'KernelMemory__Services__AzureQueue__Account' to set this
        "Account": "",
        // Required when Auth == ConnectionString
        // Note: you can use an env var 'KernelMemory__Services__AzureQueue__ConnectionString' to set this
        "ConnectionString": "",
        // Setting used only for country clouds
        "EndpointSuffix": "core.windows.net",
        // How often to check if there are new messages
        "PollDelayMsecs": 100,
        // How many messages to fetch at a time
        "FetchBatchSize": 3,
        // How long to lock messages once fetched. Azure Queue default is 30 secs
        "FetchLockSeconds": 300,
        // How many times to dequeue a messages and process before moving it to a poison queue
        "MaxRetriesBeforePoisonQueue": 20,
        // Suffix used for the poison queues.
        "PoisonQueueSuffix": "-poison"
      },
      "Elasticsearch": {
        // SHA-256 fingerprint. When running the docker image this is printed after starting the server
        // See https://www.elastic.co/guide/en/elasticsearch/reference/current/configuring-stack-security.html#_use_the_ca_fingerprint_5
        "CertificateFingerPrint": "",
        // e.g. https://localhost:9200
        "Endpoint": "",
        // e.g. "elastic"
        "UserName": "",
        "Password": "",
        "IndexPrefix": "",
        "ShardCount": 1,
        "Replicas": 0
      },
      "LlamaSharp": {
        // path to file, e.g. "llama-2-7b-chat.Q6_K.gguf"
        "ModelPath": "",
        // Max number of tokens supported by the model
        "MaxTokenTotal": 4096
        // Optional parameters
        // "GpuLayerCount": 32,
        // "Seed": 1337,
      },
      "MongoDbAtlas": {
        "ConnectionString": "mongodb://root:root@localhost:27777/?authSource=admin",
        "DatabaseName": "KernelMemory",
        "UseSingleCollectionForVectorSearch": false
      },
      "OpenAI": {
        // Name of the model used to generate text (text completion or chat completion)
        "TextModel": "gpt-3.5-turbo-16k",
        // The max number of tokens supported by the text model.
        "TextModelMaxTokenTotal": 16384,
        // What type of text generation, by default autodetect using the model name.
        // Possible values: "Auto", "TextCompletion", "Chat"
        "TextGenerationType": "Auto",
        // Name of the model used to generate text embeddings
        "EmbeddingModel": "text-embedding-ada-002",
        // The max number of tokens supported by the embedding model
        // See https://platform.openai.com/docs/guides/embeddings/what-are-embeddings
        "EmbeddingModelMaxTokenTotal": 8191,
        // OpenAI API Key
        "APIKey": "",
        // OpenAI Organization ID (usually empty, unless you have multiple accounts on different orgs)
        "OrgId": "",
        // Endpoint to use. By default the system uses 'https://api.openai.com/v1'.
        // Change this to use proxies or services compatible with OpenAI HTTP protocol like LM Studio.
        "Endpoint": "",
        // How many times to retry in case of throttling
        "MaxRetries": 10,
        // The number of dimensions output embeddings should have.
        // Only supported in "text-embedding-3" and later models developed with
        // MRL, see https://arxiv.org/abs/2205.13147
        "EmbeddingDimensions": null
      },
      "Postgres": {
        // Postgres instance connection string
        "ConnectionString": "Host=localhost;Port=5432;Username=public;Password=",
        // Mandatory prefix to add to the name of table managed by KM,
        // e.g. to exclude other tables in the same schema.
        "TableNamePrefix": "km-"
      },
      "Qdrant": {
        // Qdrant endpoint
        "Endpoint": "http://127.0.0.1:6333",
        // Qdrant API key, e.g. when using Qdrant cloud
        "APIKey": ""
      },
      "RabbitMQ": {
        "Host": "127.0.0.1",
        "Port": "5672",
        "Username": "user",
        "Password": "",
        "VirtualHost": "/",
        "MessageTTLSecs": 3600
      },
      "Redis": {
        // Redis connection string, e.g. "localhost:6379,password=..."
        "ConnectionString": "",
        // List of tags that clients will use to filter memories. When using Redis,
        // the list of tags must be configured, for data to be saved correctly.
        "Tags": {
          "type": ",",
          "user": ",",
          "ext": ","
        }
      },
      "SimpleFileStorage": {
        // Options: "Disk" or "Volatile". Volatile data is lost after each execution.
        "StorageType": "Volatile",
        // Directory where files are stored.
        "Directory": "_files"
      },
      "SimpleQueues": {
        // Options: "Disk" or "Volatile". Volatile data is lost after each execution.
        "StorageType": "Volatile",
        // Directory where files are stored.
        "Directory": "_queues"
      },
      "SimpleVectorDb": {
        // Options: "Disk" or "Volatile". Volatile data is lost after each execution.
        "StorageType": "Volatile",
        // Directory where files are stored.
        "Directory": "_vectors"
      },
      "SqlServer": {
        // MS SQL Server Connection string, e.g.
        //    "Server=tcp:127.0.0.1,1433;Initial Catalog=master;Persist Security Info=False;User ID=sa;Password=00_CHANGE_ME_00;MultipleActiveResultSets=False;TrustServerCertificate=True;Connection Timeout=30;"
        "ConnectionString": "",
        "Schema": "dbo",
        "MemoryCollectionTableName": "KMCollections",
        "MemoryTableName": "KMMemories",
        "EmbeddingsTableName": "KMEmbeddings",
        "TagsTableName": "KMMemoriesTags"
      }
    }
  }
}