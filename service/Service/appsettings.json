{
  "KernelMemory": {
    "Service": {
      // Whether to run the web service that allows to upload files and search memory
      // Use these booleans to deploy the web service and the handlers on same/different VMs
      "RunWebService": true,
      // Whether to run the asynchronous pipeline handlers
      // Use these booleans to deploy the web service and the handlers on same/different VMs
      "RunHandlers": true,
      // Whether to expose OpenAPI swagger UI at http://127.0.0.1:9001/swagger/index.html
      "OpenApiEnabled": false,
    },
    "ServiceAuthorization": {
      // Whether clients must provide some credentials to interact with the HTTP API
      "Enabled": false,
      // Currently "APIKey" is the only type supported
      "AuthenticationType": "APIKey",
      // HTTP header name to check
      "HttpHeaderName": "Authorization",
      // Define two separate API Keys, to allow key rotation. Both are active.
      // Keys must be different and case-sensitive, and at least 32 chars long.
      // Contain only alphanumeric chars and allowed symbols.
      // Symbols allowed: . _ - (dot, underscore, minus).
      "AccessKey1": "",
      "AccessKey2": "",
    },
    // "AzureBlobs" or "SimpleFileStorage"
    "ContentStorageType": "AzureBlobs",
    // "AzureOpenAIText", "OpenAI" or "LlamaSharp"
    "TextGeneratorType": "",
    // Data ingestion pipelines configuration.
    "DataIngestion": {
      // - InProcess: in process .NET orchestrator, synchronous/no queues
      // - Distributed: asynchronous queue based orchestrator
      "OrchestrationType": "Distributed",
      "DistributedOrchestration": {
        // "AzureQueues", "RabbitMQ", "SimpleQueues"
        "QueueType": "AzureQueues"
      },
      // Whether the pipeline generates and saves the vectors/embeddings in the memory DBs.
      // When using a memory DB that automatically generates embeddings internally,
      // or performs semantic search internally anyway, this should be False,
      // and avoid generating embeddings that are not used.
      // Examples:
      // * you are using Azure AI Search "semantic search" without "vector search": in this
      //   case you don't need embeddings because Azure AI Search uses a more advanced approach
      //   internally.
      // * you are using a custom Memory DB connector that generates embeddings on the fly
      //   when writing records and when searching: in this case you don't need the pipeline
      //   to calculate embeddings, because your connector does all the work.
      // * you are using a basic "text search" and a DB without "vector search": in this case
      //   embeddings would be unused, so it's better to disable them to save cost and latency.
      "EmbeddingGenerationEnabled": true,
      // Multiple generators can be used, e.g. for data migration, A/B testing, etc.
      // None of these are used for `ITextEmbeddingGeneration` dependency injection,
      // see Retrieval settings.
      "EmbeddingGeneratorTypes": [
      ],
      // Vectors can be written to multiple storages, e.g. for data migration, A/B testing, etc.
      // "AzureAISearch", "Qdrant", "Postgres", "SimpleVectorDb"
      "MemoryDbTypes": [
        "AzureAISearch"
      ],
      // "None" or "AzureAIDocIntel"
      "ImageOcrType": "None",
      // Note: keep the list empty in this file, to avoid unexpected merges
      // with the list defined in appsettings.*.json.
      // If the list is empty, KernelMemoryConfig uses 'Constants.DefaultPipeline'.
      "DefaultSteps": [
        // Default steps defined in 'Constants.DefaultPipeline'
        // "extract",
        // "partition",
        // "gen_embeddings",
        // "save_records",
      ]
    },
    "Retrieval": {
      // "AzureOpenAIEmbedding" or "OpenAI"
      // This is the generator registered for `ITextEmbeddingGeneration` dependency injection.
      "EmbeddingGeneratorType": "",
      // "AzureAISearch", "Qdrant", "Postgres", "SimpleVectorDb"
      "MemoryDbType": "AzureAISearch",
      // Search client settings
      "SearchClient": {
        // Maximum number of tokens accepted by the LLM used to generate answers.
        // The number includes the tokens used for the answer, e.g. when using
        // GPT4-32k, set this number to 32768.
        // If the value is not set or less than one, SearchClient will use the
        // max amount of tokens supported by the model in use.
        "MaxAskPromptSize": -1,
        // Maximum number of relevant sources to consider when generating an answer.
        // The value is also used as the max number of results returned by SearchAsync
        // when passing a limit less or equal to zero.
        "MaxMatchesCount": 100,
        // How many tokens to reserve for the answer generated by the LLM.
        // E.g. if the LLM supports max 4000 tokens, and AnswerTokens is 300, then
        // the prompt sent to LLM will contain max 3700 tokens, composed by
        // prompt + question + grounding information retrieved from memory.
        "AnswerTokens": 300,
        // Text to return when the LLM cannot produce an answer.
        "EmptyAnswer": "INFO NOT FOUND"
      }
    },
    "Services": {
      "SimpleFileStorage": {
        // Options: "Disk" or "Volatile". Volatile data is lost after each execution.
        "StorageType": "Volatile",
        // Directory where files are stored.
        "Directory": "_files"
      },
      "SimpleQueues": {
        // Options: "Disk" or "Volatile". Volatile data is lost after each execution.
        "StorageType": "Volatile",
        // Directory where files are stored.
        "Directory": "_queues"
      },
      "SimpleVectorDb": {
        // Options: "Disk" or "Volatile". Volatile data is lost after each execution.
        "StorageType": "Volatile",
        // Directory where files are stored.
        "Directory": "_vectors"
      },
      "AzureBlobs": {
        // "ConnectionString" or "AzureIdentity". For other options see <AzureBlobConfig>.
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        // Azure Storage account name, required when using AzureIdentity auth
        // Note: you can use an env var 'KernelMemory__Services__AzureBlobs__Account' to set this
        "Account": "",
        // Container where to create directories and upload files
        "Container": "smemory",
        // Required when Auth == ConnectionString
        // Note: you can use an env var 'KernelMemory__Services__AzureBlobs__ConnectionString' to set this
        "ConnectionString": "",
        // Setting used only for country clouds
        "EndpointSuffix": "core.windows.net"
      },
      "AzureQueues": {
        // "ConnectionString" or "AzureIdentity". For other options see <AzureQueueConfig>.
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        // Azure Storage account name, required when using AzureIdentity auth
        // Note: you can use an env var 'KernelMemory__Services__AzureQueue__Account' to set this
        "Account": "",
        // Required when Auth == ConnectionString
        // Note: you can use an env var 'KernelMemory__Services__AzureQueue__ConnectionString' to set this
        "ConnectionString": "",
        // Setting used only for country clouds
        "EndpointSuffix": "core.windows.net"
      },
      "RabbitMQ": {
        "Host": "127.0.0.1",
        "Port": "5672",
        "Username": "user",
        "Password": ""
      },
      "AzureAISearch": {
        // "ApiKey" or "AzureIdentity". For other options see <AzureAISearchConfig>.
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        "Endpoint": "https://<...>",
        "APIKey": "",
      },
      "Qdrant": {
        "Endpoint": "http://127.0.0.1:6333",
        "APIKey": "",
      },
      "Postgres": {
        // Postgres instance connection string
        "ConnectionString": "Host=localhost;Port=5432;Username=user;Password=",
        // Mandatory prefix to add to the name of table managed by KM,
        // e.g. to exclude other tables in the same schema.
        "TableNamePrefix": "km-"
      },
      "AzureOpenAIText": {
        // "ApiKey" or "AzureIdentity"
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        "Endpoint": "https://<...>.openai.azure.com/",
        "APIKey": "",
        "Deployment": "",
        // The max number of tokens supported by model deployed
        // See https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models
        "MaxTokenTotal": 16384,
        // "ChatCompletion" or "TextCompletion"
        "APIType": "ChatCompletion",
        "MaxRetries": 10
      },
      "AzureOpenAIEmbedding": {
        // "ApiKey" or "AzureIdentity"
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        "Endpoint": "https://<...>.openai.azure.com/",
        "APIKey": "",
        "Deployment": "",
        // The max number of tokens supported by model deployed
        // See https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models
        "MaxTokenTotal": 8191,
      },
      "OpenAI": {
        // Name of the model used to generate text (text completion or chat completion)
        "TextModel": "gpt-3.5-turbo-16k",
        // The max number of tokens supported by the text model.
        "TextModelMaxTokenTotal": 16384,
        // Name of the model used to generate text embeddings
        "EmbeddingModel": "text-embedding-ada-002",
        // The max number of tokens supported by the embedding model
        // See https://platform.openai.com/docs/guides/embeddings/what-are-embeddings
        "EmbeddingModelMaxTokenTotal": 8191,
        // OpenAI API Key
        "APIKey": "",
        // OpenAI Organization ID (usually empty, unless you have multiple accounts on different orgs)
        "OrgId": "",
        // How many times to retry in case of throttling
        "MaxRetries": 10
      },
      "LlamaSharp": {
        // path to file, e.g. "llama-2-7b-chat.Q6_K.gguf"
        "ModelPath": "",
        // Max number of tokens supported by the model
        "MaxTokenTotal": 4096
        // Optional parameters
        // "GpuLayerCount": 32,
        // "Seed": 1337,
      },
      "AzureAIDocIntel": {
        // "APIKey" or "AzureIdentity".
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        // Required when Auth == APIKey
        "APIKey": "",
        "Endpoint": "",
      },
    },
  },
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.KernelMemory.Pipeline.Queue.DevTools.SimpleQueue": "Information",
      // Examples: how to handle logs differently by class
      //      "Microsoft.KernelMemory.Handlers.TextExtractionHandler": "Information",
      //      "Microsoft.KernelMemory.Handlers.TextPartitioningHandler": "Information",
      //      "Microsoft.KernelMemory.Handlers.GenerateEmbeddingsHandler": "Information",
      //      "Microsoft.KernelMemory.Handlers.SaveEmbeddingsHandler": "Information",
      //      "Microsoft.KernelMemory.ContentStorage.AzureBlobs": "Information",
      //      "Microsoft.KernelMemory.Pipeline.Queue.AzureQueues": "Information",
      "Microsoft.AspNetCore": "Warning"
    }
  },
  "AllowedHosts": "*",
  "Kestrel": {
    "Endpoints": {
      "Http": {
        "Url": "http://*:9001"
      },
      "Https": {
        "Url": "https://*:9002"
      }
    }
  }
}