{
  "AllowedHosts": "*",
  "Kestrel": {
    "Endpoints": {
      "Http": {
        "Url": "http://*:9001"
      }
      // "Https": {
      //  "Url": "https://*:9002"
      // }
    }
  },
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      // Examples: how to handle logs differently by class
      //      "Microsoft.KernelMemory.Pipeline.Queue.DevTools.SimpleQueue": "Information",
      //      "Microsoft.KernelMemory.Handlers.TextExtractionHandler": "Information",
      //      "Microsoft.KernelMemory.Handlers.TextPartitioningHandler": "Information",
      //      "Microsoft.KernelMemory.Handlers.GenerateEmbeddingsHandler": "Information",
      //      "Microsoft.KernelMemory.Handlers.SaveEmbeddingsHandler": "Information",
      //      "Microsoft.KernelMemory.ContentStorage.AzureBlobs": "Information",
      //      "Microsoft.KernelMemory.Pipeline.Queue.AzureQueues": "Information",
      "Microsoft.AspNetCore": "Warning"
    }
  },
  "KernelMemory": {
    "Service": {
      // Whether to run the web service that allows to upload files and search memory
      // Use these booleans to deploy the web service and the handlers on same/different VMs
      "RunWebService": true,
      // Whether to expose OpenAPI swagger UI at http://127.0.0.1:9001/swagger/index.html
      "OpenApiEnabled": false,
      // Whether to run the asynchronous pipeline handlers
      // Use these booleans to deploy the web service and the handlers on same/different VMs
      "RunHandlers": true,
      // Handlers to load for callers (use "steps" to choose which handlers
      // to use to process a document during the ingestion)
      "Handlers": {
        // The key, e.g. "extract", is the name used when starting a pipeline with specific steps
        "extract": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.TextExtractionHandler"
        },
        "partition": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.TextPartitioningHandler"
        },
        "gen_embeddings": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.GenerateEmbeddingsHandler"
        },
        "save_records": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.SaveRecordsHandler"
        },
        "summarize": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.SummarizationHandler"
        },
        "delete_generated_files": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.DeleteGeneratedFilesHandler"
        },
        "private_delete_document": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.DeleteDocumentHandler"
        },
        "private_delete_index": {
          "Assembly": "Microsoft.KernelMemory.Core.dll",
          "Class": "Microsoft.KernelMemory.Handlers.DeleteIndexHandler"
        },
        "disabled_handler_example": {
          // Setting Class or Assembly to "" in appsettings.Development.json or appsettings.Production.json
          // allows to remove a handler defined in appsettings.json
          "Class": "",
          "Assembly": ""
        }
      }
    },
    "ServiceAuthorization": {
      // Whether clients must provide some credentials to interact with the HTTP API
      "Enabled": false,
      // Currently "APIKey" is the only type supported
      "AuthenticationType": "APIKey",
      // HTTP header name to check
      "HttpHeaderName": "Authorization",
      // Define two separate API Keys, to allow key rotation. Both are active.
      // Keys must be different and case-sensitive, and at least 32 chars long.
      // Contain only alphanumeric chars and allowed symbols.
      // Symbols allowed: . _ - (dot, underscore, minus).
      "AccessKey1": "",
      "AccessKey2": ""
    },
    // "AzureBlobs" or "SimpleFileStorage"
    "ContentStorageType": "SimpleFileStorage",
    // "AzureOpenAIText", "OpenAI" or "LlamaSharp"
    "TextGeneratorType": "",
    // Name of the index to use when none is specified
    "DefaultIndexName": "default",
    // Data ingestion pipelines configuration.
    "DataIngestion": {
      // - InProcess: in process .NET orchestrator, synchronous/no queues
      // - Distributed: asynchronous queue based orchestrator
      "OrchestrationType": "Distributed",
      "DistributedOrchestration": {
        // "AzureQueues", "RabbitMQ", "SimpleQueues"
        "QueueType": "SimpleQueues"
      },
      // Whether the pipeline generates and saves the vectors/embeddings in the memory DBs.
      // When using a memory DB that automatically generates embeddings internally,
      // or performs semantic search internally anyway, this should be False,
      // and avoid generating embeddings that are not used.
      // Examples:
      // * you are using Azure AI Search "semantic search" without "vector search": in this
      //   case you don't need embeddings because Azure AI Search uses a more advanced approach
      //   internally.
      // * you are using a custom Memory DB connector that generates embeddings on the fly
      //   when writing records and when searching: in this case you don't need the pipeline
      //   to calculate embeddings, because your connector does all the work.
      // * you are using a basic "text search" and a DB without "vector search": in this case
      //   embeddings would be unused, so it's better to disable them to save cost and latency.
      "EmbeddingGenerationEnabled": true,
      // Multiple generators can be used, e.g. for data migration, A/B testing, etc.
      // None of these are used for `ITextEmbeddingGeneration` dependency injection,
      // see Retrieval settings.
      "EmbeddingGeneratorTypes": [
      ],
      // Vectors can be written to multiple storages, e.g. for data migration, A/B testing, etc.
      // "AzureAISearch", "Qdrant", "Postgres", "Redis", "SimpleVectorDb"
      "MemoryDbTypes": [
        "SimpleVectorDb"
      ],
      // "None" or "AzureAIDocIntel"
      "ImageOcrType": "None",
      // Partitioning / Chunking settings
      // How does the partitioning work?
      // * Given a document, text is extracted, and text is split in sentences, called "lines of text".
      // * Sentences are merged into paragraphs, called "partitions".
      // * For each partition, one (potentially more) memory is generated.
      "TextPartitioning": {
        // Maximum length of lines of text (aka sentences), in tokens. Tokens depend on the LLM in use.
        // Sentences are grouped into paragraphs, see the next setting.
        "MaxTokensPerLine": 300,
        // Maximum length of paragraphs (aka partitions), in tokens. Tokens depend on the LLM in use.
        "MaxTokensPerParagraph": 1000,
        // How many tokens from a paragraph to keep in the following paragraph.
        "OverlappingTokens": 100
      },
      // Note: keep the list empty in this file, to avoid unexpected merges
      // with the list defined in appsettings.*.json.
      // If the list is empty, KernelMemoryConfig uses 'Constants.DefaultPipeline'.
      "DefaultSteps": [
        // Default steps defined in 'Constants.DefaultPipeline'
        // "extract",
        // "partition",
        // "gen_embeddings",
        // "save_records",
      ]
    },
    "Retrieval": {
      // "AzureOpenAIEmbedding" or "OpenAI"
      // This is the generator registered for `ITextEmbeddingGeneration` dependency injection.
      "EmbeddingGeneratorType": "",
      // "AzureAISearch", "Qdrant", "Postgres", "Redis", "SimpleVectorDb"
      "MemoryDbType": "SimpleVectorDb",
      // Search client settings
      "SearchClient": {
        // Maximum number of tokens accepted by the LLM used to generate answers.
        // The number includes the tokens used for the answer, e.g. when using
        // GPT4-32k, set this number to 32768.
        // If the value is not set or less than one, SearchClient will use the
        // max amount of tokens supported by the model in use.
        "MaxAskPromptSize": -1,
        // Maximum number of relevant sources to consider when generating an answer.
        // The value is also used as the max number of results returned by SearchAsync
        // when passing a limit less or equal to zero.
        "MaxMatchesCount": 100,
        // How many tokens to reserve for the answer generated by the LLM.
        // E.g. if the LLM supports max 4000 tokens, and AnswerTokens is 300, then
        // the prompt sent to LLM will contain max 3700 tokens, composed by
        // prompt + question + grounding information retrieved from memory.
        "AnswerTokens": 300,
        // Text to return when the LLM cannot produce an answer.
        "EmptyAnswer": "INFO NOT FOUND",
        // Number between 0 and 2 that controls the randomness of the completion.
        // The higher the temperature, the more random the completion.
        "Temperature": 0,
        // Number between 0 and 2 that controls the diversity of the completion.
        // The higher the TopP, the more diverse the completion.
        "TopP": 0,
        // Number between -2.0 and 2.0. Positive values penalize new tokens based on whether
        // they appear in the text so far, increasing the model's likelihood to talk about
        // new topics.
        "PresencePenalty": 0,
        // Number between -2.0 and 2.0. Positive values penalize new tokens based on their
        // existing frequency in the text so far, decreasing the model's likelihood to repeat
        // the same line verbatim.
        "FrequencyPenalty": 0,
        // Sequences where the completion will stop generating further tokens.
        "StopSequences": []
        // Modify the likelihood of specified tokens appearing in the completion.
        //"TokenSelectionBiases": { }
      }
    },
    "Services": {
      "AzureAISearch": {
        // "ApiKey" or "AzureIdentity". For other options see <AzureAISearchConfig>.
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        "Endpoint": "https://<...>",
        "APIKey": ""
      },
      "AzureAIDocIntel": {
        // "APIKey" or "AzureIdentity".
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        // Required when Auth == APIKey
        "APIKey": "",
        "Endpoint": ""
      },
      "AzureBlobs": {
        // "ConnectionString" or "AzureIdentity". For other options see <AzureBlobConfig>.
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        // Azure Storage account name, required when using AzureIdentity auth
        // Note: you can use an env var 'KernelMemory__Services__AzureBlobs__Account' to set this
        "Account": "",
        // Container where to create directories and upload files
        "Container": "smemory",
        // Required when Auth == ConnectionString
        // Note: you can use an env var 'KernelMemory__Services__AzureBlobs__ConnectionString' to set this
        "ConnectionString": "",
        // Setting used only for country clouds
        "EndpointSuffix": "core.windows.net"
      },
      "AzureOpenAIEmbedding": {
        // "ApiKey" or "AzureIdentity"
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        "Endpoint": "https://<...>.openai.azure.com/",
        "APIKey": "",
        "Deployment": "",
        // The max number of tokens supported by model deployed
        // See https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models
        "MaxTokenTotal": 8191
      },
      "AzureOpenAIText": {
        // "ApiKey" or "AzureIdentity"
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        "Endpoint": "https://<...>.openai.azure.com/",
        "APIKey": "",
        "Deployment": "",
        // The max number of tokens supported by model deployed
        // See https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models
        "MaxTokenTotal": 16384,
        // "ChatCompletion" or "TextCompletion"
        "APIType": "ChatCompletion",
        "MaxRetries": 10
      },
      "AzureQueues": {
        // "ConnectionString" or "AzureIdentity". For other options see <AzureQueueConfig>.
        // AzureIdentity: use automatic AAD authentication mechanism. You can test locally
        //   using the env vars AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET.
        "Auth": "AzureIdentity",
        // Azure Storage account name, required when using AzureIdentity auth
        // Note: you can use an env var 'KernelMemory__Services__AzureQueue__Account' to set this
        "Account": "",
        // Required when Auth == ConnectionString
        // Note: you can use an env var 'KernelMemory__Services__AzureQueue__ConnectionString' to set this
        "ConnectionString": "",
        // Setting used only for country clouds
        "EndpointSuffix": "core.windows.net"
      },
      "Elasticsearch": {
        // SHA-256 fingerprint. When running the docker image this is printed after starting the server
        // See https://www.elastic.co/guide/en/elasticsearch/reference/current/configuring-stack-security.html#_use_the_ca_fingerprint_5
        "CertificateFingerPrint": "",
        // e.g. https://localhost:9200
        "Endpoint": "",
        // e.g. "elastic"
        "UserName": "",
        "Password": "",
        "IndexPrefix": "",
        "ShardCount": 1,
        "Replicas": 0
      },
      "LlamaSharp": {
        // path to file, e.g. "llama-2-7b-chat.Q6_K.gguf"
        "ModelPath": "",
        // Max number of tokens supported by the model
        "MaxTokenTotal": 4096
        // Optional parameters
        // "GpuLayerCount": 32,
        // "Seed": 1337,
      },
      "MongoDbAtlas": {
        "ConnectionString": "mongodb://root:root@localhost:27777/?authSource=admin",
        "DatabaseName": "KernelMemory",
        "UseSingleCollectionForVectorSearch": false
      },
      "OpenAI": {
        // Name of the model used to generate text (text completion or chat completion)
        "TextModel": "gpt-3.5-turbo-16k",
        // The max number of tokens supported by the text model.
        "TextModelMaxTokenTotal": 16384,
        // What type of text generation, by default autodetect using the model name.
        // Possible values: "Auto", "TextCompletion", "Chat"
        "TextGenerationType": "Auto",
        // Name of the model used to generate text embeddings
        "EmbeddingModel": "text-embedding-ada-002",
        // The max number of tokens supported by the embedding model
        // See https://platform.openai.com/docs/guides/embeddings/what-are-embeddings
        "EmbeddingModelMaxTokenTotal": 8191,
        // OpenAI API Key
        "APIKey": "",
        // OpenAI Organization ID (usually empty, unless you have multiple accounts on different orgs)
        "OrgId": "",
        // Endpoint to use. By default the system uses 'https://api.openai.com/v1'.
        // Change this to use proxies or services compatible with OpenAI HTTP protocol like LM Studio.
        "Endpoint": "",
        // How many times to retry in case of throttling
        "MaxRetries": 10
      },
      "Postgres": {
        // Postgres instance connection string
        "ConnectionString": "Host=localhost;Port=5432;Username=public;Password=",
        // Mandatory prefix to add to the name of table managed by KM,
        // e.g. to exclude other tables in the same schema.
        "TableNamePrefix": "km-"
      },
      "Qdrant": {
        // Qdrant endpoint
        "Endpoint": "http://127.0.0.1:6333",
        // Qdrant API key, e.g. when using Qdrant cloud
        "APIKey": ""
      },
      "RabbitMQ": {
        "Host": "127.0.0.1",
        "Port": "5672",
        "Username": "user",
        "Password": "",
        "VirtualHost": "/"
      },
      "Redis": {
        // Redis connection string, e.g. "localhost:6379,password=..."
        "ConnectionString": "",
        // List of tags that clients will use to filter memories. When using Redis,
        // the list of tags must be configured, for data to be saved correctly.
        "Tags": {
          "type": ",",
          "user": ",",
          "ext": ","
        }
      },
      "SimpleFileStorage": {
        // Options: "Disk" or "Volatile". Volatile data is lost after each execution.
        "StorageType": "Volatile",
        // Directory where files are stored.
        "Directory": "_files"
      },
      "SimpleQueues": {
        // Options: "Disk" or "Volatile". Volatile data is lost after each execution.
        "StorageType": "Volatile",
        // Directory where files are stored.
        "Directory": "_queues"
      },
      "SimpleVectorDb": {
        // Options: "Disk" or "Volatile". Volatile data is lost after each execution.
        "StorageType": "Volatile",
        // Directory where files are stored.
        "Directory": "_vectors"
      }
    }
  }
}